{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NavairaRehman/Neural3DMM.git\n",
        "%cd Neural3DMM"
      ],
      "metadata": {
        "id": "5nloyt-tBNPf",
        "outputId": "6f0479e2-850b-45f6-bba7-b0f2b53c6fa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Neural3DMM'...\n",
            "remote: Enumerating objects: 115, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 115 (delta 61), reused 101 (delta 57), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (115/115), 1.30 MiB | 2.32 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "/content/Neural3DMM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Tz8CyqQZBSs8",
        "outputId": "745c6a14-edea-4c55-e071-4fa7bb356ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
            "Collecting jupyter (from -r requirements.txt (line 6))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2.18.0)\n",
            "Collecting tensorboardX (from -r requirements.txt (line 9))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting trimesh (from -r requirements.txt (line 10))\n",
            "  Downloading trimesh-4.6.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 7))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (4.25.6)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 7)) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 8)) (3.0.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (6.4.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (3.0.13)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (0.28.1)\n",
            "Collecting ipykernel (from jupyter->-r requirements.txt (line 6))\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (3.1.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (0.2.4)\n",
            "Collecting comm>=0.1.1 (from ipykernel->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (24.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.14.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.3.6)\n",
            "Collecting jupyter-client (from jupyter-console->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_events-0.11.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client->jupyter-console->jupyter->-r requirements.txt (line 6)) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.16.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.23.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.32.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->-r requirements.txt (line 6)) (2.21.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 6)) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.22.3)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (24.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.12.2)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.6.1-py3-none-any.whl (707 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m707.0/707.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.11.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, trimesh, tensorboardX, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, json5, jedi, fqdn, comm, async-lru, jupyter-server-terminals, jupyter-client, arrow, isoduration, ipykernel, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 async-lru-2.0.4 comm-0.2.2 fqdn-1.5.1 ipykernel-6.29.5 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.11.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.5 jupyterlab-server-2.27.3 overrides-7.7.0 python-json-logger-3.2.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 tensorboardX-2.6.2.2 trimesh-4.6.1 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/MPI-IS/mesh.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cjRxdPvcB4ZF",
        "outputId": "7a7698cb-6459-44b7-aac5-e9928b0c1d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/MPI-IS/mesh.git\n",
            "  Cloning https://github.com/MPI-IS/mesh.git to /tmp/pip-req-build-indkni4m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MPI-IS/mesh.git /tmp/pip-req-build-indkni4m\n",
            "  Resolved https://github.com/MPI-IS/mesh.git to commit 49e70425cf373ec5269917012bda2944215c5ccd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (4.10.0.84)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (11.1.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (3.1.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (6.0.2)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (24.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (1.13.1)\n",
            "Building wheels for collected packages: psbody-mesh\n",
            "  Building wheel for psbody-mesh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psbody-mesh: filename=psbody_mesh-0.4-cp311-cp311-linux_x86_64.whl size=2321656 sha256=caf7376f6817e9208c8e758d7a2426e490520fdc162a877a205c0313a0796d56\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vnds_qbm/wheels/f9/3c/ac/b3d1c84ec8972b9c7d1bf82a23432a142fa6bb4fba62c6c4d8\n",
            "Successfully built psbody-mesh\n",
            "Installing collected packages: psbody-mesh\n",
            "Successfully installed psbody-mesh-0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepping for COMA"
      ],
      "metadata": {
        "id": "8bIBicX2pfa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Neural3DMM\n",
        "!mkdir root\n",
        "%cd root\n",
        "!mkdir COMA\n",
        "%cd COMA\n",
        "!mkdir preprocessed\n",
        "!mkdir template\n",
        "%cd preprocessed"
      ],
      "metadata": {
        "id": "XgKU5RCWplDp",
        "outputId": "98ccf213-9090-4ddb-c8bf-562dce4858e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Neural3DMM'\n",
            "/content/Neural3DMM\n",
            "/content/Neural3DMM/root\n",
            "/content/Neural3DMM/root/COMA\n",
            "/content/Neural3DMM/root/COMA/preprocessed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qfzOxeI2pfJ5",
        "outputId": "f8bc4475-64ea-4df8-adac-28c724797138",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copy train.npy and test.npy from MyDrive to /content/Neural3DMM/root/COMA/preprocessed\n",
        "import shutil\n",
        "shutil.copy('/content/drive/MyDrive/train.npy', '/content/Neural3DMM/root/COMA/preprocessed')\n",
        "shutil.copy('/content/drive/MyDrive/test.npy', '/content/Neural3DMM/root/COMA/preprocessed')"
      ],
      "metadata": {
        "id": "vCBhmmjfqqn1",
        "outputId": "0d52a5d0-b406-4648-b91a-2e3708527c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Neural3DMM/root/COMA/preprocessed/test.npy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd template\n",
        "!mkdir COMA_downsample\n",
        "%cd COMA_downsample"
      ],
      "metadata": {
        "id": "jeWsoBN1sDlX",
        "outputId": "1c64f8ed-2e8c-477e-c9f5-4f9364f13e7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Neural3DMM/root/COMA\n",
            "/content/Neural3DMM/root/COMA/template\n",
            "/content/Neural3DMM/root/COMA/template/COMA_downsample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remaining"
      ],
      "metadata": {
        "id": "Fw548At1sK_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/root_data.zip -d /content/root_data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rQiXe_ASCcgJ",
        "outputId": "5d3730f4-02ea-457b-eb33-425275b1d942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/root_data.zip\n",
            "   creating: /content/root_data/root_data/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/preprocessed/\n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/mean.npy  \n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/std.npy  \n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/test.npy  \n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/train.npy  \n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/checkpoints/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/predictions/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/samples/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/summaries/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/template/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/template/meshlab_downsample/\n",
            "  inflating: /content/root_data/root_data/Facial_Norms/template/template.obj  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Neural3DMM"
      ],
      "metadata": {
        "id": "IxaZmliJENvt",
        "outputId": "27656072-b205-492c-cbda-658650e570e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Neural3DMM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pt7zaX5f_2f5",
        "outputId": "bf2cbda8-20ca-4f43-cd5e-7b63d64792d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import mesh_sampling\n",
        "import trimesh\n",
        "from shape_data import ShapeData\n",
        "\n",
        "from autoencoder_dataset import autoencoder_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from spiral_utils import get_adj_trigs, generate_spirals\n",
        "from models import SpiralAutoencoder\n",
        "from train_funcs import train_autoencoder_dataloader\n",
        "from test_funcs import test_autoencoder_dataloader\n",
        "\n",
        "\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "meshpackage = 'mpi-mesh' # 'mpi-mesh', 'trimesh'\n",
        "root_dir = '/content/Neural3DMM/root'\n",
        "\n",
        "dataset = 'COMA'\n",
        "name = ''\n",
        "\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "torch.cuda.get_device_name(device_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G_SoWOfP_2f7"
      },
      "outputs": [],
      "source": [
        "args = {}\n",
        "\n",
        "generative_model = 'autoencoder'\n",
        "downsample_method = 'COMA_downsample' # choose'COMA_downsample' or 'meshlab_downsample'\n",
        "\n",
        "\n",
        "# below are the arguments for the DFAUST run\n",
        "reference_mesh_file = os.path.join(root_dir, dataset, 'template', 'template.obj')\n",
        "downsample_directory = os.path.join(root_dir, dataset,'template', downsample_method)\n",
        "ds_factors = [4, 4, 4, 4]\n",
        "step_sizes = [2, 2, 1, 1, 1]\n",
        "filter_sizes_enc = [[3, 16, 32, 64, 128],[[],[],[],[],[]]]\n",
        "filter_sizes_dec = [[128, 64, 32, 32, 16],[[],[],[],[],3]]\n",
        "dilation_flag = True\n",
        "if dilation_flag:\n",
        "    dilation=[2, 2, 1, 1, 1]\n",
        "else:\n",
        "    dilation = None\n",
        "reference_points = [[414]]  # [[3567,4051,4597]] used for COMA with 3 disconnected components\n",
        "\n",
        "args = {'generative_model': generative_model,\n",
        "        'name': name, 'data': os.path.join(root_dir, dataset, 'preprocessed',name),\n",
        "        'results_folder':  os.path.join(root_dir, dataset,'results/spirals_'+ generative_model),\n",
        "        'reference_mesh_file':reference_mesh_file, 'downsample_directory': downsample_directory,\n",
        "        'checkpoint_file': 'checkpoint',\n",
        "        'seed':2, 'loss':'l1',\n",
        "        'batch_size':16, 'num_epochs':300, 'eval_frequency':200, 'num_workers': 4,\n",
        "        'filter_sizes_enc': filter_sizes_enc, 'filter_sizes_dec': filter_sizes_dec,\n",
        "        'nz':16,\n",
        "        'ds_factors': ds_factors, 'step_sizes' : step_sizes, 'dilation': dilation,\n",
        "\n",
        "        'lr':1e-3,\n",
        "        'regularization': 5e-5,\n",
        "        'scheduler': True, 'decay_rate': 0.99,'decay_steps':1,\n",
        "        'resume': False,\n",
        "\n",
        "        'mode':'train', 'shuffle': True, 'nVal': 100, 'normalization': True}\n",
        "\n",
        "args['results_folder'] = os.path.join(args['results_folder'],'latent_'+str(args['nz']))\n",
        "\n",
        "if not os.path.exists(os.path.join(args['results_folder'])):\n",
        "    os.makedirs(os.path.join(args['results_folder']))\n",
        "\n",
        "summary_path = os.path.join(args['results_folder'],'summaries',args['name'])\n",
        "if not os.path.exists(summary_path):\n",
        "    os.makedirs(summary_path)\n",
        "\n",
        "checkpoint_path = os.path.join(args['results_folder'],'checkpoints', args['name'])\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)\n",
        "\n",
        "samples_path = os.path.join(args['results_folder'],'samples', args['name'])\n",
        "if not os.path.exists(samples_path):\n",
        "    os.makedirs(samples_path)\n",
        "\n",
        "prediction_path = os.path.join(args['results_folder'],'predictions', args['name'])\n",
        "if not os.path.exists(prediction_path):\n",
        "    os.makedirs(prediction_path)\n",
        "\n",
        "if not os.path.exists(downsample_directory):\n",
        "    os.makedirs(downsample_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pm7tjQEu_2f9",
        "outputId": "fad2e7f4-8e8b-417c-9b81-d370164a2b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data .. \n",
            "Loading Transform Matrices ..\n",
            "Calculating reference points for downsampled versions..\n"
          ]
        }
      ],
      "source": [
        "from psbody.mesh import Mesh\n",
        "import mesh_sampling\n",
        "\n",
        "np.random.seed(args['seed'])\n",
        "print(\"Loading data .. \")\n",
        "if not os.path.exists(args['data']+'/mean.npy') or not os.path.exists(args['data']+'/std.npy'):\n",
        "    shapedata =  ShapeData(nVal=args['nVal'],\n",
        "                          train_file=args['data']+'/train.npy',\n",
        "                          test_file=args['data']+'/test.npy',\n",
        "                          reference_mesh_file=args['reference_mesh_file'],\n",
        "                          normalization = args['normalization'],\n",
        "                          meshpackage = meshpackage, load_flag = True)\n",
        "    np.save(args['data']+'/mean.npy', shapedata.mean)\n",
        "    np.save(args['data']+'/std.npy', shapedata.std)\n",
        "else:\n",
        "    shapedata = ShapeData(nVal=args['nVal'],\n",
        "                         train_file=args['data']+'/train.npy',\n",
        "                         test_file=args['data']+'/test.npy',\n",
        "                         reference_mesh_file=args['reference_mesh_file'],\n",
        "                         normalization = args['normalization'],\n",
        "                         meshpackage = meshpackage, load_flag = False)\n",
        "    shapedata.mean = np.load(args['data']+'/mean.npy')\n",
        "    shapedata.std = np.load(args['data']+'/std.npy')\n",
        "    shapedata.n_vertex = shapedata.mean.shape[0]\n",
        "    shapedata.n_features = shapedata.mean.shape[1]\n",
        "\n",
        "if not os.path.exists(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl')):\n",
        "    if shapedata.meshpackage == 'trimesh':\n",
        "        raise NotImplementedError('Rerun with mpi-mesh as meshpackage')\n",
        "    print(\"Generating Transform Matrices ..\")\n",
        "    if downsample_method == 'COMA_downsample':\n",
        "        M,A,D,U,F = mesh_sampling.generate_transform_matrices(shapedata.reference_mesh, args['ds_factors'])\n",
        "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'wb') as fp:\n",
        "        M_verts_faces = [(M[i].v, M[i].f) for i in range(len(M))]\n",
        "        pickle.dump({'M_verts_faces':M_verts_faces,'A':A,'D':D,'U':U,'F':F}, fp)\n",
        "else:\n",
        "    print(\"Loading Transform Matrices ..\")\n",
        "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'rb') as fp:\n",
        "        #downsampling_matrices = pickle.load(fp,encoding = 'latin1')\n",
        "        downsampling_matrices = pickle.load(fp)\n",
        "\n",
        "    M_verts_faces = downsampling_matrices['M_verts_faces']\n",
        "    if shapedata.meshpackage == 'mpi-mesh':\n",
        "        M = [Mesh(v=M_verts_faces[i][0], f=M_verts_faces[i][1]) for i in range(len(M_verts_faces))]\n",
        "    elif shapedata.meshpackage == 'trimesh':\n",
        "        M = [trimesh.base.Trimesh(vertices=M_verts_faces[i][0], faces=M_verts_faces[i][1], process = False) for i in range(len(M_verts_faces))]\n",
        "    A = downsampling_matrices['A']\n",
        "    D = downsampling_matrices['D']\n",
        "    U = downsampling_matrices['U']\n",
        "    F = downsampling_matrices['F']\n",
        "\n",
        "# Needs also an extra check to enforce points to belong to different disconnected component at each hierarchy level\n",
        "print(\"Calculating reference points for downsampled versions..\")\n",
        "for i in range(len(args['ds_factors'])):\n",
        "    if shapedata.meshpackage == 'mpi-mesh':\n",
        "        dist = euclidean_distances(M[i+1].v, M[0].v[reference_points[0]])\n",
        "    elif shapedata.meshpackage == 'trimesh':\n",
        "        dist = euclidean_distances(M[i+1].vertices, M[0].vertices[reference_points[0]])\n",
        "    reference_points.append(np.argmin(dist,axis=0).tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "id": "ePo-Ozpa_2f-",
        "outputId": "51b16ae6-9743-47f4-a153-0e4683242492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spiral generation for hierarchy 0 (5023 vertices) finished\n",
            "spiral generation for hierarchy 1 (1256 vertices) finished\n",
            "spiral generation for hierarchy 2 (314 vertices) finished\n",
            "spiral generation for hierarchy 3 (79 vertices) finished\n",
            "spiral generation for hierarchy 4 (20 vertices) finished\n",
            "spiral sizes for hierarchy 0:  16\n",
            "spiral sizes for hierarchy 1:  16\n",
            "spiral sizes for hierarchy 2:  10\n",
            "spiral sizes for hierarchy 3:  10\n",
            "spiral sizes for hierarchy 4:  8\n"
          ]
        }
      ],
      "source": [
        "if shapedata.meshpackage == 'mpi-mesh':\n",
        "    sizes = [x.v.shape[0] for x in M]\n",
        "elif shapedata.meshpackage == 'trimesh':\n",
        "    sizes = [x.vertices.shape[0] for x in M]\n",
        "Adj, Trigs = get_adj_trigs(A, F, shapedata.reference_mesh, meshpackage = shapedata.meshpackage)\n",
        "\n",
        "spirals_np, spiral_sizes,spirals = generate_spirals(args['step_sizes'],\n",
        "                                                    M, Adj, Trigs,\n",
        "                                                    reference_points = reference_points,\n",
        "                                                    dilation = args['dilation'], random = False,\n",
        "                                                    meshpackage = shapedata.meshpackage,\n",
        "                                                    counter_clockwise = True)\n",
        "\n",
        "bU = []\n",
        "bD = []\n",
        "for i in range(len(D)):\n",
        "    d = np.zeros((1,D[i].shape[0]+1,D[i].shape[1]+1))\n",
        "    u = np.zeros((1,U[i].shape[0]+1,U[i].shape[1]+1))\n",
        "    d[0,:-1,:-1] = D[i].todense()\n",
        "    u[0,:-1,:-1] = U[i].todense()\n",
        "    d[0,-1,-1] = 1\n",
        "    u[0,-1,-1] = 1\n",
        "    bD.append(d)\n",
        "    bU.append(u)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xP1dVvbv_2f_",
        "outputId": "460e6558-bb86-4909-9528-f4b7a9e7146f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(args['seed'])\n",
        "\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "tspirals = [torch.from_numpy(s).long().to(device) for s in spirals_np]\n",
        "tD = [torch.from_numpy(s).float().to(device) for s in bD]\n",
        "tU = [torch.from_numpy(s).float().to(device) for s in bU]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_generation.py --root_dir=/content/Neural3DMM/root --dataset=COMA --num_valid=500"
      ],
      "metadata": {
        "id": "YE6swaWxE-4q",
        "outputId": "47ad8540-7b9b-40a3-fd14-7eeee3f21b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 17915/17915 [00:09<00:00, 1910.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the root folder to run locally\n",
        "#zip root using gz\n",
        "import tarfile\n",
        "\n",
        "def compress_directory_with_gz(directory_path, output_path):\n",
        "    with tarfile.open(output_path, \"w:gz\") as tar:\n",
        "        tar.add(directory_path, arcname=\".\")\n",
        "\n",
        "# Usage example:\n",
        "directory_to_compress = \"/content/Neural3DMM/root\"\n",
        "output_gz_file = \"/content/archive.tar.gz\"\n",
        "compress_directory_with_gz(directory_to_compress, output_gz_file)\n"
      ],
      "metadata": {
        "id": "wRJU3LV6tdiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def convert_bytes(size):\n",
        "    \"\"\"Convert bytes to KB, MB, or GB.\"\"\"\n",
        "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if size < 1024:\n",
        "            return f\"{size:.2f} {unit}\"\n",
        "        size /= 1024\n",
        "\n",
        "# Replace 'your_file_path' with the actual path to your file\n",
        "file_path = '/content/root.zip'\n",
        "\n",
        "try:\n",
        "    # Get the file size in bytes\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    # Convert the file size to a human-readable format\n",
        "    readable_size = convert_bytes(file_size)\n",
        "    print(f\"The size of the file is: {readable_size}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the file path.\")\n",
        "except OSError:\n",
        "    print(\"An error occurred while accessing the file.\")\n"
      ],
      "metadata": {
        "id": "r-ZCXwsVnxX2",
        "outputId": "9a979a52-2bc9-4d3f-8a0b-c8337f6d0a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the file is: 2.11 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your file\n",
        "files.download('/content/root.zip')"
      ],
      "metadata": {
        "id": "nKbBkA9ZohEg",
        "outputId": "648c6deb-3dee-40dd-d0fa-ebdb29db8757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e0c08d7e-2972-4f8e-8fc5-5c8f834408e3\", \"root.zip\", 2268334711)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LFzeQCNk_2f_",
        "outputId": "93256cc2-de45-4c90-916c-21cc66485d0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Building model, optimizer, and loss function\n",
        "\n",
        "dataset_train = autoencoder_dataset(root_dir = args['data'], points_dataset = 'train',\n",
        "                                           shapedata = shapedata,\n",
        "                                           normalization = args['normalization'])\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=args['batch_size'],\\\n",
        "                                     shuffle = args['shuffle'], num_workers = args['num_workers'])\n",
        "\n",
        "dataset_val = autoencoder_dataset(root_dir = args['data'], points_dataset = 'val',\n",
        "                                         shapedata = shapedata,\n",
        "                                         normalization = args['normalization'])\n",
        "\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=args['batch_size'],\\\n",
        "                                     shuffle = False, num_workers = args['num_workers'])\n",
        "\n",
        "\n",
        "dataset_test = autoencoder_dataset(root_dir = args['data'], points_dataset = 'test',\n",
        "                                          shapedata = shapedata,\n",
        "                                          normalization = args['normalization'])\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=args['batch_size'],\\\n",
        "                                     shuffle = False, num_workers = args['num_workers'])\n",
        "\n",
        "\n",
        "\n",
        "if 'autoencoder' in args['generative_model']:\n",
        "        model = SpiralAutoencoder(filters_enc = args['filter_sizes_enc'],\n",
        "                                  filters_dec = args['filter_sizes_dec'],\n",
        "                                  latent_size=args['nz'],\n",
        "                                  sizes=sizes,\n",
        "                                  spiral_sizes=spiral_sizes,\n",
        "                                  spirals=tspirals,\n",
        "                                  D=tD, U=tU,device=device).to(device)\n",
        "\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(),lr=args['lr'],weight_decay=args['regularization'])\n",
        "if args['scheduler']:\n",
        "    scheduler=torch.optim.lr_scheduler.StepLR(optim, args['decay_steps'],gamma=args['decay_rate'])\n",
        "else:\n",
        "    scheduler = None\n",
        "\n",
        "if args['loss']=='l1':\n",
        "    def loss_l1(outputs, targets):\n",
        "        L = torch.abs(outputs - targets).mean()\n",
        "        return L\n",
        "    loss_fn = loss_l1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XfgxE_xM_2gA",
        "outputId": "ec9fe7c3-280a-4a4f-8be0-cb76863271c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters is: 328211\n",
            "SpiralAutoencoder(\n",
            "  (encoder): SpiralEncoder(\n",
            "    (conv): ModuleList(\n",
            "      (0): SpiralConv(\n",
            "        (conv): Linear(in_features=48, out_features=16, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (1): SpiralConv(\n",
            "        (conv): Linear(in_features=256, out_features=32, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (2): SpiralConv(\n",
            "        (conv): Linear(in_features=320, out_features=64, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (3): SpiralConv(\n",
            "        (conv): Linear(in_features=640, out_features=128, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "    )\n",
            "    (fc_latent_enc): Linear(in_features=2688, out_features=16, bias=True)\n",
            "  )\n",
            "  (decoder): SpiralDecoder(\n",
            "    (fc_latent_dec): Linear(in_features=16, out_features=2688, bias=True)\n",
            "    (dconv): ModuleList(\n",
            "      (0): SpiralConv(\n",
            "        (conv): Linear(in_features=1280, out_features=64, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (1): SpiralConv(\n",
            "        (conv): Linear(in_features=640, out_features=32, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (2): SpiralConv(\n",
            "        (conv): Linear(in_features=512, out_features=32, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (3): SpiralConv(\n",
            "        (conv): Linear(in_features=512, out_features=16, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (4): SpiralConv(\n",
            "        (conv): Linear(in_features=256, out_features=3, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters is: {}\".format(params))\n",
        "print(model)\n",
        "# print(M[4].v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jAH4EB6Q_2gA",
        "outputId": "c72cdd10-f5cb-418f-d461-58bf1bf36fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1120 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1120/1120 [01:18<00:00, 14.34it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 26.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 | tr 0.3037162799368416 | val 0.181982364654541\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_22631/3163981256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generative_model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'autoencoder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         train_autoencoder_dataloader(dataloader_train, dataloader_val,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                       \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                       \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Neural3DMM/train_funcs.py\u001b[0m in \u001b[0;36mtrain_autoencoder_dataloader\u001b[0;34m(dataloader_train, dataloader_val, device, model, optim, loss_fn, start_epoch, n_epochs, eval_freq, scheduler, writer, save_recons, shapedata, metadata_dir, samples_dir, checkpoint_path)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mmesh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mmsh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmesh_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0mshapedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_meshes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'epoch_{epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tx' is not defined"
          ]
        }
      ],
      "source": [
        "if args['mode'] == 'train':\n",
        "    writer = SummaryWriter(summary_path)\n",
        "    with open(os.path.join(args['results_folder'], 'checkpoints', args['name'] + '_params.json'), 'w') as fp:\n",
        "        saveparams = copy.deepcopy(args)\n",
        "        json.dump(saveparams, fp)\n",
        "\n",
        "    if args['resume']:\n",
        "        print('loading checkpoint from file %s' % (os.path.join(checkpoint_path, args['checkpoint_file'])))\n",
        "        checkpoint_dict = torch.load(os.path.join(checkpoint_path, args['checkpoint_file'] + '.pth.tar'), map_location=device)\n",
        "        start_epoch = checkpoint_dict['epoch'] + 1\n",
        "        model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
        "        optim.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint_dict['scheduler_state_dict'])\n",
        "        print('Resuming from epoch %s' % (str(start_epoch)))\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    if args['generative_model'] == 'autoencoder':\n",
        "        train_autoencoder_dataloader(dataloader_train, dataloader_val,\n",
        "                                      device, model, optim, loss_fn,\n",
        "                                      start_epoch=start_epoch,\n",
        "                                      n_epochs=args['num_epochs'],\n",
        "                                      eval_freq=args['eval_frequency'],\n",
        "                                      scheduler=scheduler,\n",
        "                                      writer=writer,\n",
        "                                      save_recons=True,\n",
        "                                      shapedata=shapedata,\n",
        "                                      metadata_dir=checkpoint_path,\n",
        "                                      samples_dir=samples_path,\n",
        "                                      checkpoint_path=args['checkpoint_file'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgialpEY_2gB"
      },
      "outputs": [],
      "source": [
        "if args['mode'] == 'test':\n",
        "    print('loading checkpoint from file %s'%(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar')))\n",
        "    checkpoint_dict = torch.load(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar'),map_location=device)\n",
        "    model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
        "\n",
        "    predictions, norm_l1_loss, l2_loss = test_autoencoder_dataloader(device, model, dataloader_test,\n",
        "                                                                     shapedata, mm_constant = 1000)\n",
        "    np.save(os.path.join(prediction_path,'predictions'), predictions)\n",
        "\n",
        "    print('autoencoder: normalized loss', norm_l1_loss)\n",
        "\n",
        "    print('autoencoder: euclidean distance in mm=', l2_loss)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}