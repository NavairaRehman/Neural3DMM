{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NavairaRehman/Neural3DMM.git\n",
        "%cd Neural3DMM"
      ],
      "metadata": {
        "id": "5nloyt-tBNPf",
        "outputId": "0201f8e9-95e4-4b4e-9b1a-70b39207b240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Neural3DMM'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 129 (delta 69), reused 112 (delta 63), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (129/129), 1.31 MiB | 3.50 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n",
            "/content/Neural3DMM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Tz8CyqQZBSs8",
        "outputId": "8350b57d-a967-4927-9b40-5aed7cb4aa04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
            "Collecting jupyter (from -r requirements.txt (line 6))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2.18.0)\n",
            "Collecting tensorboardX (from -r requirements.txt (line 9))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting trimesh (from -r requirements.txt (line 10))\n",
            "  Downloading trimesh-4.6.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter->-r requirements.txt (line 6)) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 7))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (4.25.6)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 7)) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 8)) (3.0.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (6.4.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 6)) (3.0.13)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (0.28.1)\n",
            "Collecting ipykernel (from jupyter->-r requirements.txt (line 6))\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (3.1.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 6)) (0.2.4)\n",
            "Collecting comm>=0.1.1 (from ipykernel->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 6)) (24.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 6)) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.14.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.3.6)\n",
            "Collecting jupyter-client (from jupyter-console->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_events-0.11.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client->jupyter-console->jupyter->-r requirements.txt (line 6)) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.16.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.23.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.32.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->-r requirements.txt (line 6)) (2.21.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 6)) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (0.22.3)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r requirements.txt (line 6)) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6)) (24.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 6)) (4.12.2)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 6))\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.6.1-py3-none-any.whl (707 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m707.0/707.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.11.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, trimesh, tensorboardX, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, json5, jedi, fqdn, comm, async-lru, jupyter-server-terminals, jupyter-client, arrow, isoduration, ipykernel, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 async-lru-2.0.4 comm-0.2.2 fqdn-1.5.1 ipykernel-6.29.5 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.11.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.5 jupyterlab-server-2.27.3 overrides-7.7.0 python-json-logger-3.2.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 tensorboardX-2.6.2.2 trimesh-4.6.1 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/MPI-IS/mesh.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cjRxdPvcB4ZF",
        "outputId": "e8a52ecc-c810-4a6f-a87b-ba6201422e0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/MPI-IS/mesh.git\n",
            "  Cloning https://github.com/MPI-IS/mesh.git to /tmp/pip-req-build-td27t36w\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MPI-IS/mesh.git /tmp/pip-req-build-td27t36w\n",
            "  Resolved https://github.com/MPI-IS/mesh.git to commit 49e70425cf373ec5269917012bda2944215c5ccd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (4.10.0.84)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (11.1.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (3.1.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (6.0.2)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (24.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from psbody-mesh==0.4) (1.13.1)\n",
            "Building wheels for collected packages: psbody-mesh\n",
            "  Building wheel for psbody-mesh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psbody-mesh: filename=psbody_mesh-0.4-cp311-cp311-linux_x86_64.whl size=2321667 sha256=dc6119f8bb763ee2a61e7406cef0a630652a0482601339a5d5ba316e0e3d21b5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3x65wfmc/wheels/f9/3c/ac/b3d1c84ec8972b9c7d1bf82a23432a142fa6bb4fba62c6c4d8\n",
            "Successfully built psbody-mesh\n",
            "Installing collected packages: psbody-mesh\n",
            "Successfully installed psbody-mesh-0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepping for COMA"
      ],
      "metadata": {
        "id": "8bIBicX2pfa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Neural3DMM\n",
        "!mkdir root\n",
        "%cd root\n",
        "!mkdir COMA\n",
        "%cd COMA\n",
        "!mkdir preprocessed\n",
        "!mkdir template\n",
        "%cd preprocessed"
      ],
      "metadata": {
        "id": "XgKU5RCWplDp",
        "outputId": "0e75fa80-383a-47ac-cc06-7a318b9f7175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Neural3DMM'\n",
            "/content/Neural3DMM/root/COMA/preprocessed\n",
            "/content/Neural3DMM/root/COMA/preprocessed/root\n",
            "/content/Neural3DMM/root/COMA/preprocessed/root/COMA\n",
            "/content/Neural3DMM/root/COMA/preprocessed/root/COMA/preprocessed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qfzOxeI2pfJ5",
        "outputId": "aae2c2c3-7396-4b31-bab2-1f3d584bd4eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copy train.npy and test.npy from MyDrive to /content/Neural3DMM/root/COMA/preprocessed\n",
        "import shutil\n",
        "shutil.copy('/content/drive/MyDrive/train.npy', '/content/Neural3DMM/root/COMA/preprocessed')\n",
        "shutil.copy('/content/drive/MyDrive/test.npy', '/content/Neural3DMM/root/COMA/preprocessed')"
      ],
      "metadata": {
        "id": "vCBhmmjfqqn1",
        "outputId": "e30dd13a-714e-42f6-847a-91f947593d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Neural3DMM/root/COMA/preprocessed/test.npy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd template\n",
        "!mkdir COMA_downsample\n",
        "%cd COMA_downsample"
      ],
      "metadata": {
        "id": "jeWsoBN1sDlX",
        "outputId": "94ae1c41-80d0-43eb-8482-7280964360a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Neural3DMM/root/COMA/preprocessed/root/COMA\n",
            "/content/Neural3DMM/root/COMA/preprocessed/root/COMA/template\n",
            "/content/Neural3DMM/root/COMA/preprocessed/root/COMA/template/COMA_downsample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remaining"
      ],
      "metadata": {
        "id": "Fw548At1sK_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/root_data.zip -d /content/root_data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rQiXe_ASCcgJ",
        "outputId": "5d3730f4-02ea-457b-eb33-425275b1d942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/root_data.zip\n",
            "   creating: /content/root_data/root_data/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/preprocessed/\n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/mean.npy  \n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/std.npy  \n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/test.npy  \n",
            "  inflating: /content/root_data/root_data/Facial_Norms/preprocessed/train.npy  \n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/checkpoints/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/predictions/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/samples/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/results/spirals_autoencoder/latent_16/summaries/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/template/\n",
            "   creating: /content/root_data/root_data/Facial_Norms/template/meshlab_downsample/\n",
            "  inflating: /content/root_data/root_data/Facial_Norms/template/template.obj  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Neural3DMM"
      ],
      "metadata": {
        "id": "IxaZmliJENvt",
        "outputId": "6fb2bba2-b756-4a5e-a039-da1ef63ff5e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Neural3DMM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pt7zaX5f_2f5",
        "outputId": "368ab296-484d-4cb9-f2e4-770330888fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import mesh_sampling\n",
        "import trimesh\n",
        "from shape_data import ShapeData\n",
        "\n",
        "from autoencoder_dataset import autoencoder_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from spiral_utils import get_adj_trigs, generate_spirals\n",
        "from models import SpiralAutoencoder\n",
        "from train_funcs import train_autoencoder_dataloader\n",
        "from test_funcs import test_autoencoder_dataloader\n",
        "\n",
        "\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "meshpackage = 'mpi-mesh' # 'mpi-mesh', 'trimesh'\n",
        "root_dir = '/content/Neural3DMM/root'\n",
        "\n",
        "dataset = 'COMA'\n",
        "name = ''\n",
        "\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "torch.cuda.get_device_name(device_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G_SoWOfP_2f7"
      },
      "outputs": [],
      "source": [
        "args = {}\n",
        "\n",
        "generative_model = 'autoencoder'\n",
        "downsample_method = 'COMA_downsample' # choose'COMA_downsample' or 'meshlab_downsample'\n",
        "\n",
        "\n",
        "# below are the arguments for the DFAUST run\n",
        "reference_mesh_file = os.path.join(root_dir, dataset, 'template', 'template.obj')\n",
        "downsample_directory = os.path.join(root_dir, dataset,'template', downsample_method)\n",
        "ds_factors = [4, 4, 4, 4]\n",
        "step_sizes = [2, 2, 1, 1, 1]\n",
        "filter_sizes_enc = [[3, 16, 32, 64, 128],[[],[],[],[],[]]]\n",
        "filter_sizes_dec = [[128, 64, 32, 32, 16],[[],[],[],[],3]]\n",
        "dilation_flag = True\n",
        "if dilation_flag:\n",
        "    dilation=[2, 2, 1, 1, 1]\n",
        "else:\n",
        "    dilation = None\n",
        "reference_points = [[414]]  # [[3567,4051,4597]] used for COMA with 3 disconnected components\n",
        "\n",
        "args = {'generative_model': generative_model,\n",
        "        'name': name, 'data': os.path.join(root_dir, dataset, 'preprocessed',name),\n",
        "        'results_folder':  os.path.join(root_dir, dataset,'results/spirals_'+ generative_model),\n",
        "        'reference_mesh_file':reference_mesh_file, 'downsample_directory': downsample_directory,\n",
        "        'checkpoint_file': 'checkpoint',\n",
        "        'seed':2, 'loss':'l1',\n",
        "        'batch_size':16, 'num_epochs':300, 'eval_frequency':200, 'num_workers': 4,\n",
        "        'filter_sizes_enc': filter_sizes_enc, 'filter_sizes_dec': filter_sizes_dec,\n",
        "        'nz':16,\n",
        "        'ds_factors': ds_factors, 'step_sizes' : step_sizes, 'dilation': dilation,\n",
        "\n",
        "        'lr':1e-3,\n",
        "        'regularization': 5e-5,\n",
        "        'scheduler': True, 'decay_rate': 0.99,'decay_steps':1,\n",
        "        'resume': True,\n",
        "\n",
        "        'mode':'train', 'shuffle': True, 'nVal': 100, 'normalization': True}\n",
        "\n",
        "args['results_folder'] = os.path.join(args['results_folder'],'latent_'+str(args['nz']))\n",
        "\n",
        "if not os.path.exists(os.path.join(args['results_folder'])):\n",
        "    os.makedirs(os.path.join(args['results_folder']))\n",
        "\n",
        "summary_path = os.path.join(args['results_folder'],'summaries',args['name'])\n",
        "if not os.path.exists(summary_path):\n",
        "    os.makedirs(summary_path)\n",
        "\n",
        "checkpoint_path = os.path.join(args['results_folder'],'checkpoints', args['name'])\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)\n",
        "\n",
        "samples_path = os.path.join(args['results_folder'],'samples', args['name'])\n",
        "if not os.path.exists(samples_path):\n",
        "    os.makedirs(samples_path)\n",
        "\n",
        "prediction_path = os.path.join(args['results_folder'],'predictions', args['name'])\n",
        "if not os.path.exists(prediction_path):\n",
        "    os.makedirs(prediction_path)\n",
        "\n",
        "if not os.path.exists(downsample_directory):\n",
        "    os.makedirs(downsample_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pm7tjQEu_2f9",
        "outputId": "4e5a9910-00d9-456b-9472-aae9b44be688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data .. \n",
            "Loading Transform Matrices ..\n",
            "Calculating reference points for downsampled versions..\n"
          ]
        }
      ],
      "source": [
        "from psbody.mesh import Mesh\n",
        "import mesh_sampling\n",
        "\n",
        "np.random.seed(args['seed'])\n",
        "print(\"Loading data .. \")\n",
        "if not os.path.exists(args['data']+'/mean.npy') or not os.path.exists(args['data']+'/std.npy'):\n",
        "    shapedata =  ShapeData(nVal=args['nVal'],\n",
        "                          train_file=args['data']+'/train.npy',\n",
        "                          test_file=args['data']+'/test.npy',\n",
        "                          reference_mesh_file=args['reference_mesh_file'],\n",
        "                          normalization = args['normalization'],\n",
        "                          meshpackage = meshpackage, load_flag = True)\n",
        "    np.save(args['data']+'/mean.npy', shapedata.mean)\n",
        "    np.save(args['data']+'/std.npy', shapedata.std)\n",
        "else:\n",
        "    shapedata = ShapeData(nVal=args['nVal'],\n",
        "                         train_file=args['data']+'/train.npy',\n",
        "                         test_file=args['data']+'/test.npy',\n",
        "                         reference_mesh_file=args['reference_mesh_file'],\n",
        "                         normalization = args['normalization'],\n",
        "                         meshpackage = meshpackage, load_flag = False)\n",
        "    shapedata.mean = np.load(args['data']+'/mean.npy')\n",
        "    shapedata.std = np.load(args['data']+'/std.npy')\n",
        "    shapedata.n_vertex = shapedata.mean.shape[0]\n",
        "    shapedata.n_features = shapedata.mean.shape[1]\n",
        "\n",
        "if not os.path.exists(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl')):\n",
        "    if shapedata.meshpackage == 'trimesh':\n",
        "        raise NotImplementedError('Rerun with mpi-mesh as meshpackage')\n",
        "    print(\"Generating Transform Matrices ..\")\n",
        "    if downsample_method == 'COMA_downsample':\n",
        "        M,A,D,U,F = mesh_sampling.generate_transform_matrices(shapedata.reference_mesh, args['ds_factors'])\n",
        "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'wb') as fp:\n",
        "        M_verts_faces = [(M[i].v, M[i].f) for i in range(len(M))]\n",
        "        pickle.dump({'M_verts_faces':M_verts_faces,'A':A,'D':D,'U':U,'F':F}, fp)\n",
        "else:\n",
        "    print(\"Loading Transform Matrices ..\")\n",
        "    with open(os.path.join(args['downsample_directory'],'downsampling_matrices.pkl'), 'rb') as fp:\n",
        "        #downsampling_matrices = pickle.load(fp,encoding = 'latin1')\n",
        "        downsampling_matrices = pickle.load(fp)\n",
        "\n",
        "    M_verts_faces = downsampling_matrices['M_verts_faces']\n",
        "    if shapedata.meshpackage == 'mpi-mesh':\n",
        "        M = [Mesh(v=M_verts_faces[i][0], f=M_verts_faces[i][1]) for i in range(len(M_verts_faces))]\n",
        "    elif shapedata.meshpackage == 'trimesh':\n",
        "        M = [trimesh.base.Trimesh(vertices=M_verts_faces[i][0], faces=M_verts_faces[i][1], process = False) for i in range(len(M_verts_faces))]\n",
        "    A = downsampling_matrices['A']\n",
        "    D = downsampling_matrices['D']\n",
        "    U = downsampling_matrices['U']\n",
        "    F = downsampling_matrices['F']\n",
        "\n",
        "# Needs also an extra check to enforce points to belong to different disconnected component at each hierarchy level\n",
        "print(\"Calculating reference points for downsampled versions..\")\n",
        "for i in range(len(args['ds_factors'])):\n",
        "    if shapedata.meshpackage == 'mpi-mesh':\n",
        "        dist = euclidean_distances(M[i+1].v, M[0].v[reference_points[0]])\n",
        "    elif shapedata.meshpackage == 'trimesh':\n",
        "        dist = euclidean_distances(M[i+1].vertices, M[0].vertices[reference_points[0]])\n",
        "    reference_points.append(np.argmin(dist,axis=0).tolist())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "ePo-Ozpa_2f-",
        "outputId": "9b74f3bb-dd29-416f-87cc-208d82d27021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spiral generation for hierarchy 0 (5023 vertices) finished\n",
            "spiral generation for hierarchy 1 (1256 vertices) finished\n",
            "spiral generation for hierarchy 2 (314 vertices) finished\n",
            "spiral generation for hierarchy 3 (79 vertices) finished\n",
            "spiral generation for hierarchy 4 (20 vertices) finished\n",
            "spiral sizes for hierarchy 0:  16\n",
            "spiral sizes for hierarchy 1:  16\n",
            "spiral sizes for hierarchy 2:  10\n",
            "spiral sizes for hierarchy 3:  10\n",
            "spiral sizes for hierarchy 4:  8\n"
          ]
        }
      ],
      "source": [
        "if shapedata.meshpackage == 'mpi-mesh':\n",
        "    sizes = [x.v.shape[0] for x in M]\n",
        "elif shapedata.meshpackage == 'trimesh':\n",
        "    sizes = [x.vertices.shape[0] for x in M]\n",
        "Adj, Trigs = get_adj_trigs(A, F, shapedata.reference_mesh, meshpackage = shapedata.meshpackage)\n",
        "\n",
        "spirals_np, spiral_sizes,spirals = generate_spirals(args['step_sizes'],\n",
        "                                                    M, Adj, Trigs,\n",
        "                                                    reference_points = reference_points,\n",
        "                                                    dilation = args['dilation'], random = False,\n",
        "                                                    meshpackage = shapedata.meshpackage,\n",
        "                                                    counter_clockwise = True)\n",
        "\n",
        "bU = []\n",
        "bD = []\n",
        "for i in range(len(D)):\n",
        "    d = np.zeros((1,D[i].shape[0]+1,D[i].shape[1]+1))\n",
        "    u = np.zeros((1,U[i].shape[0]+1,U[i].shape[1]+1))\n",
        "    d[0,:-1,:-1] = D[i].todense()\n",
        "    u[0,:-1,:-1] = U[i].todense()\n",
        "    d[0,-1,-1] = 1\n",
        "    u[0,-1,-1] = 1\n",
        "    bD.append(d)\n",
        "    bU.append(u)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xP1dVvbv_2f_",
        "outputId": "9449e428-ff1b-4979-a5f0-37f352d7fadd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(args['seed'])\n",
        "\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\"+str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "tspirals = [torch.from_numpy(s).long().to(device) for s in spirals_np]\n",
        "tD = [torch.from_numpy(s).float().to(device) for s in bD]\n",
        "tU = [torch.from_numpy(s).float().to(device) for s in bU]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_generation.py --root_dir=/content/Neural3DMM/root --dataset=COMA --num_valid=500"
      ],
      "metadata": {
        "id": "YE6swaWxE-4q",
        "outputId": "f9901c8d-8e81-496a-f56e-da7fb47c610b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 17915/17915 [00:03<00:00, 4653.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the root folder to run locally\n",
        "#zip root using gz\n",
        "import tarfile\n",
        "\n",
        "def compress_directory_with_gz(directory_path, output_path):\n",
        "    with tarfile.open(output_path, \"w:gz\") as tar:\n",
        "        tar.add(directory_path, arcname=\".\")\n",
        "\n",
        "# Usage example:\n",
        "directory_to_compress = \"/content/Neural3DMM/root\"\n",
        "output_gz_file = \"/content/archive.tar.gz\"\n",
        "compress_directory_with_gz(directory_to_compress, output_gz_file)\n"
      ],
      "metadata": {
        "id": "wRJU3LV6tdiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def convert_bytes(size):\n",
        "    \"\"\"Convert bytes to KB, MB, or GB.\"\"\"\n",
        "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if size < 1024:\n",
        "            return f\"{size:.2f} {unit}\"\n",
        "        size /= 1024\n",
        "\n",
        "# Replace 'your_file_path' with the actual path to your file\n",
        "file_path = '/content/root.zip'\n",
        "\n",
        "try:\n",
        "    # Get the file size in bytes\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    # Convert the file size to a human-readable format\n",
        "    readable_size = convert_bytes(file_size)\n",
        "    print(f\"The size of the file is: {readable_size}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the file path.\")\n",
        "except OSError:\n",
        "    print(\"An error occurred while accessing the file.\")\n"
      ],
      "metadata": {
        "id": "r-ZCXwsVnxX2",
        "outputId": "9a979a52-2bc9-4d3f-8a0b-c8337f6d0a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the file is: 2.11 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Replace 'your_file.txt' with the path to your file\n",
        "files.download('/content/root.zip')"
      ],
      "metadata": {
        "id": "nKbBkA9ZohEg",
        "outputId": "648c6deb-3dee-40dd-d0fa-ebdb29db8757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e0c08d7e-2972-4f8e-8fc5-5c8f834408e3\", \"root.zip\", 2268334711)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LFzeQCNk_2f_",
        "outputId": "2e6a1c83-d2d4-410c-cc4b-2021bac4cf87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Building model, optimizer, and loss function\n",
        "\n",
        "dataset_train = autoencoder_dataset(root_dir = args['data'], points_dataset = 'train',\n",
        "                                           shapedata = shapedata,\n",
        "                                           normalization = args['normalization'])\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=args['batch_size'],\\\n",
        "                                     shuffle = args['shuffle'], num_workers = args['num_workers'])\n",
        "\n",
        "dataset_val = autoencoder_dataset(root_dir = args['data'], points_dataset = 'val',\n",
        "                                         shapedata = shapedata,\n",
        "                                         normalization = args['normalization'])\n",
        "\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=args['batch_size'],\\\n",
        "                                     shuffle = False, num_workers = args['num_workers'])\n",
        "\n",
        "\n",
        "dataset_test = autoencoder_dataset(root_dir = args['data'], points_dataset = 'test',\n",
        "                                          shapedata = shapedata,\n",
        "                                          normalization = args['normalization'])\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=args['batch_size'],\\\n",
        "                                     shuffle = False, num_workers = args['num_workers'])\n",
        "\n",
        "\n",
        "\n",
        "if 'autoencoder' in args['generative_model']:\n",
        "        model = SpiralAutoencoder(filters_enc = args['filter_sizes_enc'],\n",
        "                                  filters_dec = args['filter_sizes_dec'],\n",
        "                                  latent_size=args['nz'],\n",
        "                                  sizes=sizes,\n",
        "                                  spiral_sizes=spiral_sizes,\n",
        "                                  spirals=tspirals,\n",
        "                                  D=tD, U=tU,device=device).to(device)\n",
        "\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(),lr=args['lr'],weight_decay=args['regularization'])\n",
        "if args['scheduler']:\n",
        "    scheduler=torch.optim.lr_scheduler.StepLR(optim, args['decay_steps'],gamma=args['decay_rate'])\n",
        "else:\n",
        "    scheduler = None\n",
        "\n",
        "if args['loss']=='l1':\n",
        "    def loss_l1(outputs, targets):\n",
        "        L = torch.abs(outputs - targets).mean()\n",
        "        return L\n",
        "    loss_fn = loss_l1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XfgxE_xM_2gA",
        "outputId": "3c787af2-3b53-4f1d-b07f-1c1f88c4e35a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters is: 328211\n",
            "SpiralAutoencoder(\n",
            "  (encoder): SpiralEncoder(\n",
            "    (conv): ModuleList(\n",
            "      (0): SpiralConv(\n",
            "        (conv): Linear(in_features=48, out_features=16, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (1): SpiralConv(\n",
            "        (conv): Linear(in_features=256, out_features=32, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (2): SpiralConv(\n",
            "        (conv): Linear(in_features=320, out_features=64, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (3): SpiralConv(\n",
            "        (conv): Linear(in_features=640, out_features=128, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "    )\n",
            "    (fc_latent_enc): Linear(in_features=2688, out_features=16, bias=True)\n",
            "  )\n",
            "  (decoder): SpiralDecoder(\n",
            "    (fc_latent_dec): Linear(in_features=16, out_features=2688, bias=True)\n",
            "    (dconv): ModuleList(\n",
            "      (0): SpiralConv(\n",
            "        (conv): Linear(in_features=1280, out_features=64, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (1): SpiralConv(\n",
            "        (conv): Linear(in_features=640, out_features=32, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (2): SpiralConv(\n",
            "        (conv): Linear(in_features=512, out_features=32, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (3): SpiralConv(\n",
            "        (conv): Linear(in_features=512, out_features=16, bias=True)\n",
            "        (activation): ELU(alpha=1.0)\n",
            "      )\n",
            "      (4): SpiralConv(\n",
            "        (conv): Linear(in_features=256, out_features=3, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters is: {}\".format(params))\n",
        "print(model)\n",
        "# print(M[4].v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jAH4EB6Q_2gA",
        "outputId": "5ec74957-3845-40aa-ebf3-4414c4745d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-0d082b2d3f79>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(os.path.join(checkpoint_path, args['checkpoint_file'] + '.pth.tar'), map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading checkpoint from file /content/Neural3DMM/root/COMA/results/spirals_autoencoder/latent_16/checkpoints/checkpoint\n",
            "Resuming from epoch 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1120 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1120/1120 [02:31<00:00,  7.41it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 56 | tr 0.0958184806479777 | val 0.12973478710651398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.43it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 57 | tr 0.09579498040095541 | val 0.13555878961086273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.49it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 58 | tr 0.09550069307327737 | val 0.128481516122818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:28<00:00,  7.52it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 59 | tr 0.09520939730280377 | val 0.13064788275957107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.49it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 60 | tr 0.09521164318602278 | val 0.1253829140663147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:30<00:00,  7.45it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 61 | tr 0.0949705765000642 | val 0.13696143734455107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.49it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 62 | tr 0.09476043782668708 | val 0.12713212954998016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:32<00:00,  7.34it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 63 | tr 0.09442540213919005 | val 0.13085365295410156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.51it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 64 | tr 0.09437382106312886 | val 0.13226431810855865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.49it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 65 | tr 0.09408871692641967 | val 0.1286000987291336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.46it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 66 | tr 0.09403720774191104 | val 0.12413089287281036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.50it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 67 | tr 0.09360552546683883 | val 0.12522833287715912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.47it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 19.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 68 | tr 0.0937752185492851 | val 0.12776245725154878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.49it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 69 | tr 0.09310905582705856 | val 0.12630426466464997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 14.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 70 | tr 0.09366816212528417 | val 0.129406962454319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.43it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 71 | tr 0.09290491075213649 | val 0.12766606920957566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.51it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 15.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 72 | tr 0.09271482945969076 | val 0.1293861237168312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 19.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 73 | tr 0.09255675668411005 | val 0.12674474549293518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 74 | tr 0.09233299852240993 | val 0.12773624259233474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.44it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 75 | tr 0.09220305358626607 | val 0.13116008061170578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:30<00:00,  7.47it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 76 | tr 0.09229229768286396 | val 0.13007316958904266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:31<00:00,  7.41it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 77 | tr 0.09194927458203507 | val 0.1247475101351738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.50it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 78 | tr 0.09205303249239688 | val 0.13185864007472992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 79 | tr 0.09147784826378233 | val 0.12858167243003846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 80 | tr 0.0916589627406832 | val 0.1248756827712059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.47it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 81 | tr 0.09120459915488384 | val 0.12397932553291321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.49it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 82 | tr 0.09095994622521716 | val 0.13078266239166259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.49it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 15.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 83 | tr 0.09093732825710958 | val 0.12685318344831467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.50it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 84 | tr 0.09096540540930881 | val 0.1289759864807129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.51it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 85 | tr 0.090536755254789 | val 0.12984858447313308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:30<00:00,  7.46it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 14.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 86 | tr 0.09066629199352205 | val 0.12514863622188568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.44it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 87 | tr 0.09024084712992891 | val 0.12411965918540954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.47it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 88 | tr 0.09035017593401688 | val 0.12425140738487243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:31<00:00,  7.41it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 18.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 89 | tr 0.09029929436454219 | val 0.12770046496391296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.47it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 19.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 90 | tr 0.08983282318061668 | val 0.1293504095673561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:28<00:00,  7.55it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 91 | tr 0.0899399536371464 | val 0.12378333294391632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.47it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 15.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 92 | tr 0.08961711796034391 | val 0.12385635286569595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.45it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 19.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 93 | tr 0.08963839656400707 | val 0.12537985640764238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.43it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 16.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 94 | tr 0.08953376216634572 | val 0.130662691116333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 95 | tr 0.08923773392099194 | val 0.12396991181373596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.47it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 19.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 96 | tr 0.08910575459622617 | val 0.12522575223445892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:01<00:00, 17.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 97 | tr 0.08901976082633069 | val 0.1254280077815056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:30<00:00,  7.45it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 98 | tr 0.08895846682738699 | val 0.13305174684524537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.45it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 14.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 99 | tr 0.08876869315803966 | val 0.12465510028600693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:30<00:00,  7.46it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 100 | tr 0.08865230000371745 | val 0.12312894439697265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:28<00:00,  7.52it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 101 | tr 0.08841530660361242 | val 0.12555474108457565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:29<00:00,  7.51it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 102 | tr 0.08836308500885198 | val 0.12617695635557175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.50it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 103 | tr 0.08833255832741921 | val 0.1272577708363533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1120/1120 [02:29<00:00,  7.48it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 12.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 104 | tr 0.0881726993600849 | val 0.12741490453481674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:28<00:00,  7.54it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 105 | tr 0.08792577483843333 | val 0.13126377534866332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1120/1120 [02:30<00:00,  7.42it/s]\n",
            "100%|██████████| 32/32 [00:02<00:00, 13.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 106 | tr 0.08810584750374768 | val 0.12863211435079575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 923/1120 [02:03<00:26,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0d082b2d3f79>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generative_model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'autoencoder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         train_autoencoder_dataloader(dataloader_train, dataloader_val,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                       \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                       \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Neural3DMM/train_funcs.py\u001b[0m in \u001b[0;36mtrain_autoencoder_dataloader\u001b[0;34m(dataloader_train, dataloader_val, device, model, optim, loss_fn, start_epoch, n_epochs, eval_freq, scheduler, writer, save_recons, shapedata, metadata_dir, samples_dir, checkpoint_path)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Neural3DMM/train_funcs.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader_train, model, optim, loss_fn, shapedata, device, writer, eval_freq, total_steps)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'points'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mtx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Neural3DMM/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_recon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Neural3DMM/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspiral_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Neural3DMM/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, spiral_adj)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mout_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspirals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mout_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if args['mode'] == 'train':\n",
        "    writer = SummaryWriter(summary_path)\n",
        "    with open(os.path.join(args['results_folder'], 'checkpoints', args['name'] + '_params.json'), 'w') as fp:\n",
        "        saveparams = copy.deepcopy(args)\n",
        "        json.dump(saveparams, fp)\n",
        "\n",
        "    if args['resume']:\n",
        "        print('loading checkpoint from file %s' % (os.path.join(checkpoint_path, args['checkpoint_file'])))\n",
        "        checkpoint_dict = torch.load(os.path.join(checkpoint_path, args['checkpoint_file'] + '.pth.tar'), map_location=device)\n",
        "        start_epoch = checkpoint_dict['epoch'] + 1\n",
        "        model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
        "        optim.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint_dict['scheduler_state_dict'])\n",
        "        print('Resuming from epoch %s' % (str(start_epoch)))\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    if args['generative_model'] == 'autoencoder':\n",
        "        train_autoencoder_dataloader(dataloader_train, dataloader_val,\n",
        "                                      device, model, optim, loss_fn,\n",
        "                                      start_epoch=start_epoch,\n",
        "                                      n_epochs=args['num_epochs'],\n",
        "                                      eval_freq=args['eval_frequency'],\n",
        "                                      scheduler=scheduler,\n",
        "                                      writer=writer,\n",
        "                                      save_recons=True,\n",
        "                                      shapedata=shapedata,\n",
        "                                      metadata_dir=checkpoint_path,\n",
        "                                      samples_dir=samples_path,\n",
        "                                      checkpoint_path=args['checkpoint_file'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgialpEY_2gB"
      },
      "outputs": [],
      "source": [
        "if args['mode'] == 'test':\n",
        "    print('loading checkpoint from file %s'%(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar')))\n",
        "    checkpoint_dict = torch.load(os.path.join(checkpoint_path,args['checkpoint_file']+'.pth.tar'),map_location=device)\n",
        "    model.load_state_dict(checkpoint_dict['autoencoder_state_dict'])\n",
        "\n",
        "    predictions, norm_l1_loss, l2_loss = test_autoencoder_dataloader(device, model, dataloader_test,\n",
        "                                                                     shapedata, mm_constant = 1000)\n",
        "    np.save(os.path.join(prediction_path,'predictions'), predictions)\n",
        "\n",
        "    print('autoencoder: normalized loss', norm_l1_loss)\n",
        "\n",
        "    print('autoencoder: euclidean distance in mm=', l2_loss)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}